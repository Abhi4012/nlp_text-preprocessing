{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61636e4f",
   "metadata": {},
   "source": [
    "## Nlp Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad391a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # importing regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "906a629e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, this is a small paragraph with HTML tags. I am using the strong tag to make some text bold, and the em tag to make some text italic. I can also use the  tag to insert a line break.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def html_remove(text):\n",
    "    html_tag = re.sub(r'<.*?>|\\d+', '', text)\n",
    "    return html_tag\n",
    "\n",
    "data = \"\"\"<p>Hello, this is a small paragraph with HTML tags. I am using the <strong>strong</strong> tag to make some text bold, and the <em>em</em> tag to make some text italic. I can also use the <br> tag to insert a line break.</p>\"\"\"\n",
    "result = html_remove(data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6149c10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Hi this is test of remove url link!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def url_remove(text):\n",
    "    url_tag = re.sub(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\", '', text)\n",
    "    return url_tag\n",
    "txt_data = \"\"\"https://www.w3schools.com/\"Hi this is test of remove url link!\"\"\"\n",
    "result2 = url_remove(txt_data)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0141f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The bustling city with its towering skyscrapers and crowded streets hums with energy and excitement People rush to and fro immersed in the rhythm of urban life The honking of cars the chatter of pedestrians and the occasional sirens create a symphony of city sounds'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def punc_remove(text):\n",
    "    punc_tags = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "    return punc_tags\n",
    "punc_data = \"\"\"\"The bustling city?, with its towering skyscrapers and crowded streets, hums with energy and excitement!. People rush to and fro, immersed in the rhythm of urban life. The honking of cars, the chatter of pedestrians, and the occasional sirens create a symphony of city sounds.\"\"\"\n",
    "punc_result = punc_remove(punc_data)\n",
    "punc_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cdcf6",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57ace8",
   "metadata": {},
   "source": [
    "- Sentence tokenize using `nltk` and `spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3e175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68abca31",
   "metadata": {},
   "source": [
    "**Sentence tokenize using Nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b707be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is example of sentence tokenize.',\n",
       " 'It is use to convert corpus into meaningfull sentence.',\n",
       " 'it is a part of text preprocessing.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_tokenize(sent):\n",
    "    sentence = sent_tokenize(sent)\n",
    "    return sentence\n",
    "sent_data = \"This is example of sentence tokenize. It is use to convert corpus into meaningfull sentence. it is a part of text preprocessing.\"\n",
    "sent_result = sent_tokenize(sent_data)\n",
    "sent_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47dad4",
   "metadata": {},
   "source": [
    "**sentence Tokenization using Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c494318d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is use to convert corpus into meaningfull sentence.',\n",
       " 'it is a part of text preprocessing.',\n",
       " 'This is example of sentence tokenize.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_sen_token(text):\n",
    "    doc = nlp(text)\n",
    "    sentence = [sent.text for sent in doc.sents]\n",
    "    return sentence\n",
    "spacy_sen_txt = \"It is use to convert corpus into meaningfull sentence. it is a part of text preprocessing. This is example of sentence tokenize.\"\n",
    "spacy_result = spacy_sen_token(spacy_sen_txt)\n",
    "spacy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b9245",
   "metadata": {},
   "source": [
    "**word tokenize**\n",
    "- word tokenize using `nltk` and `spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dd43bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d1fc1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'example', 'of', 'word', 'tokenization', 'using', 'nltk']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_token(text):\n",
    "    word = word_tokenize(text)\n",
    "    return word\n",
    "word_token_data = \"This is example of word tokenization using nltk\"\n",
    "result_word_tokenize = word_token(word_token_data)\n",
    "result_word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c91853b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'example',\n",
       " 'of',\n",
       " 'word',\n",
       " 'tokenize',\n",
       " 'using',\n",
       " 'spacy',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerfull',\n",
       " 'techinique',\n",
       " 'of',\n",
       " 'word',\n",
       " 'tokenize']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def word_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    words = [word.text for word in doc]\n",
    "    return words\n",
    "spacy_word_data = \"This is example of word tokenize using spacy and it is a powerfull techinique of word tokenize\"\n",
    "spacy_word_token_result = word_spacy(spacy_word_data)\n",
    "spacy_word_token_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524212e",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904f997",
   "metadata": {},
   "source": [
    "**Remove stop words using nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9431d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ak352\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ak352\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce00f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'word', 'tokenize', 'using', 'spacy', 'powerfull', 'techinique', 'word', 'tokenize']\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_words(words):\n",
    "    stop_words = (stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "original_tokens = ['This','is','example','of','word','tokenize','using','spacy','and','it','is','a','powerfull','techinique','of','word','tokenize']\n",
    "\n",
    "result_of_stopword = remove_stop_words(original_tokens)\n",
    "print(result_of_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdb03c",
   "metadata": {},
   "source": [
    "**Remove stop words using spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e70edda0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bustling',\n",
       " 'city',\n",
       " 'towering',\n",
       " 'skyscrapers',\n",
       " 'crowded',\n",
       " 'streets',\n",
       " 'hums',\n",
       " 'energy',\n",
       " 'excitement',\n",
       " 'People',\n",
       " 'rush',\n",
       " 'fro',\n",
       " 'immersed',\n",
       " 'rhythm',\n",
       " 'urban',\n",
       " 'life',\n",
       " 'honking',\n",
       " 'cars',\n",
       " 'chatter',\n",
       " 'pedestrians',\n",
       " 'occasional',\n",
       " 'sirens',\n",
       " 'create',\n",
       " 'symphony',\n",
       " 'city',\n",
       " 'sounds']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def remove_stopword_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "    return filtered_words\n",
    "\n",
    "original_data = \"The bustling city with its towering skyscrapers and crowded streets hums with energy and excitement People rush to and fro immersed in the rhythm of urban life The honking of cars the chatter of pedestrians and the occasional sirens create a symphony of city sounds\"\n",
    "spacy_stop_word = remove_stopword_spacy(original_data)\n",
    "spacy_stop_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e77b1d",
   "metadata": {},
   "source": [
    "**Remove stopwords using custom stopwords list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2e626c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'custom', 'data']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_stop_word(text, custom_stopword):\n",
    "    words = word_tokenize(text)\n",
    "    word_filter = [word for word in words if word.lower() not in custom_stopword]\n",
    "    return word_filter\n",
    "\n",
    "custom_data = ['a', 'in', 'and', 'not', 'is']\n",
    "\n",
    "data_text = 'This is not a custom data'\n",
    "result_custom = custom_stop_word(data_text, custom_data)\n",
    "result_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc879d9a",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "- It will give root word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fdc150",
   "metadata": {},
   "source": [
    "**Stemming using Porterstemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb06e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histori\n",
      "stori\n",
      "stem\n",
      "go\n",
      "rune\n",
      "cri\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "def port_stem(text):\n",
    "    stems = PorterStemmer()\n",
    "    stemm_word = stems.stem(text)\n",
    "    return stemm_word\n",
    "\n",
    "stem_text = ['history', 'stories', 'stemming', 'going', 'runing', 'cried']\n",
    "for i in stem_text:\n",
    "    print(port_stem(i))\n",
    "# stem_result = port_stem(stem_text)\n",
    "# stem_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773fbc3",
   "metadata": {},
   "source": [
    "**Porterstemmer using spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4caee79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "contribut\n",
      "histori\n",
      "stori\n",
      "stem\n",
      "go\n",
      "rune\n",
      "cri\n",
      "gone\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_port_stem(text):\n",
    "    doc = nlp(text)\n",
    "    port_stem = PorterStemmer()\n",
    "    stemm_sen = ' '.join([port_stem.stem(token.text) for token in doc])\n",
    "    return stemm_sen\n",
    "stem_text = ['loving','contributed','history', 'stories', 'stemming', 'going', 'runing', 'cried', 'gone']\n",
    "for word in stem_text:\n",
    "    print(spacy_port_stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28c89b",
   "metadata": {},
   "source": [
    "**Stemming using Lancaster Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b6569ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lov\n",
      "contribut\n",
      "hist\n",
      "story\n",
      "stem\n",
      "going\n",
      "run\n",
      "cri\n",
      "gon\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "def lanc_stemmer(text):\n",
    "    lanc_stem = LancasterStemmer()\n",
    "    word_lancstem = lanc_stem.stem(text)\n",
    "    return word_lancstem\n",
    "\n",
    "lanc_text = ['loving','contributed','history', 'stories', 'stemming', 'going', 'runing', 'cried', 'gone']\n",
    "for j in stem_text:\n",
    "    print(lanc_stemmer(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a5b76",
   "metadata": {},
   "source": [
    "**Stemming using Lancaster Stemmer with spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "244fab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lov\n",
      "contribut\n",
      "hist\n",
      "story\n",
      "stem\n",
      "going\n",
      "run\n",
      "cri\n",
      "gon\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def lanc_stem_spacy(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    lanc_stem = LancasterStemmer()\n",
    "    lanc_sentence = ' '.join([lanc_stem.stem(token.text) for token in doc])\n",
    "    return lanc_sentence\n",
    "\n",
    "lancstem_text = ['loving','contributed','history', 'stories', 'stemming', 'going', 'runing', 'cried', 'gone']\n",
    "for k in lancstem_text:\n",
    "    print(lanc_stem_spacy(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1c049",
   "metadata": {},
   "source": [
    "**Stemming using Snowball Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73785a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baby\n",
      "lov\n",
      "contribut\n",
      "hist\n",
      "story\n",
      "stem\n",
      "going\n",
      "run\n",
      "cri\n",
      "gon\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def snow_ball_stemmer(sent, language='english'):\n",
    "    snow_ball = SnowballStemmer(language)\n",
    "    sent_snowball = snow_ball.stem(sent)\n",
    "    return sent_snowball\n",
    "\n",
    "\n",
    "data = ['babies','loved','contributed','history', 'stories', 'stemming', 'going', 'runing', 'cried', 'gone']\n",
    "for a in data:\n",
    "    print(lanc_stem_spacy(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faa0ea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babi\n",
      "love\n",
      "contribut\n",
      "histori\n",
      "stori\n",
      "stem\n",
      "go\n",
      "rune\n",
      "cri\n",
      "gone\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def snow_ball_stemmer(sent, language='english'):\n",
    "    snow_ball = SnowballStemmer(language)\n",
    "    sent_snowball = snow_ball.stem(sent)\n",
    "    return sent_snowball\n",
    "\n",
    "data = ['babies','loved','contributed','history', 'stories', 'stemming', 'going', 'runing', 'cried', 'gone']\n",
    "for word in data:\n",
    "    print(snow_ball_stemmer(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20769f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from nltk.stem import SnowballStemmer\n",
    "\n",
    "# def snow_ball_stemmer(sent, language='english'):\n",
    "#     snow_ball = SnowballStemmer(language)\n",
    "#     sent_snowball = snow_ball.stem(sent)\n",
    "#     return sent_snowball\n",
    "\n",
    "# data = ['kites', 'babies', 'dogs', 'flying', 'smiling', 'driving', 'died', 'tried', 'feet']\n",
    "\n",
    "# df = pd.DataFrame({'original_word': data, 'stem_word': [snow_ball_stemmer(word) for word in data]})\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea640ad",
   "metadata": {},
   "source": [
    "**Stemming using Snowball Stemmer with spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53b9b5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babi\n",
      "love\n",
      "contribut\n",
      "histori\n",
      "stori\n",
      "stem\n",
      "go\n",
      "rune\n",
      "cri\n",
      "gone\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def snow_ball_stemmer_spacy(sent):\n",
    "    doc = nlp(sent)\n",
    "    snow_ball = SnowballStemmer('english')\n",
    "    snow_ball_stem_sent = ' '.join([snow_ball.stem(token.text) for token in doc])\n",
    "    return snow_ball_stem_sent\n",
    "\n",
    "data_sent = ['babies','loved','contributed','history', 'stories', 'stemming', 'going', 'runing', 'cried', 'gone']\n",
    "for b in data_sent:\n",
    "    print(snow_ball_stemmer_spacy(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae73a5a",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526bdc4",
   "metadata": {},
   "source": [
    "**many techinique of lemmatization**\n",
    "   - WordNet Lemmatizer\n",
    "   - Spacy Lemmatization\n",
    "   - NLTK Lemmatization\n",
    "   - Stanford Lemmatizer etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0526880d",
   "metadata": {},
   "source": [
    "**wordnet lemma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4cb3f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baby\n",
      "country\n",
      "loved\n",
      "contributed\n",
      "history\n",
      "story\n",
      "stemming\n",
      "going\n",
      "runing\n",
      "cried\n",
      "gone\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def word_lemma(word):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    sent_lemma = wordnet.lemmatize(word)\n",
    "    return sent_lemma\n",
    "data = ['babies','countries','loved','contributed','histories', 'stories', 'stemming', 'going', 'runing', 'cried', 'gone']\n",
    "\n",
    "final_result = [word_lemma(word) for word in data]\n",
    "for result in final_result:\n",
    "    print(word_lemma(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15fa613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glass\n",
      "baby\n",
      "country\n",
      "history\n",
      "story\n",
      "word\n",
      "go\n",
      "run\n",
      "carry\n",
      "goal\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def word_lemma(sentence):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    words = sentence.split()\n",
    "    sent_lemma = ' '.join([wordnet.lemmatize(word) for word in words])\n",
    "    return sent_lemma\n",
    "\n",
    "norm_data = ['glasses','babies','countries','histories', 'stories', 'words', 'goes', 'runs', 'carries', 'goals']\n",
    "for c in norm_data:\n",
    "    print(word_lemma(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccfd44",
   "metadata": {},
   "source": [
    "**Lemma using spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5606a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baby\n",
      "country\n",
      "love\n",
      "contribute\n",
      "history\n",
      "story\n",
      "stem\n",
      "go\n",
      "run\n",
      "cry\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def lemma_using_spacy(sent):\n",
    "    doc = nlp(sent)\n",
    "    lemma_sent = ' '.join([token.lemma_ for token in doc])\n",
    "    return lemma_sent\n",
    "\n",
    "sentences = ['babies','countries','loved','contributed','histories', 'stories', 'stemming', 'going', 'runing', 'cried', 'gone']\n",
    "for i in sentences:\n",
    "    print(lemma_using_spacy(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4ce48",
   "metadata": {},
   "source": [
    "**Lemma using textbolo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b124d077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baby\n",
      "country\n",
      "love\n",
      "contributed\n",
      "history\n",
      "story\n",
      "stemming\n",
      "going\n",
      "runing\n",
      "carry\n",
      "gone\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "def lemma_using_blob(text):\n",
    "    blob_obj = TextBlob(text)\n",
    "    blob_lemma_text = ' '.join([i.lemmatize() for i in blob_obj.words])\n",
    "    return blob_lemma_text\n",
    "\n",
    "sent = ['babies','countries','loves','contributed','histories', 'stories', 'stemming', 'going', 'runing', 'carries', 'gone']\n",
    "for i in sent:\n",
    "    print(lemma_using_blob(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3662d",
   "metadata": {},
   "source": [
    "**Lemma using gensim, simple_preprocess and spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d06d17c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baby\n",
      "country\n",
      "love\n",
      "contribute\n",
      "history\n",
      "story\n",
      "stem\n",
      "go\n",
      "run\n",
      "carry\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def gen_lemma(text):\n",
    "    doc = nlp(text)\n",
    "    gensim_word_lemma = simple_preprocess(text)\n",
    "    gensim_word_lemma = ' '.join([token.lemma_ for token in doc])\n",
    "    return gensim_word_lemma\n",
    "\n",
    "sent = ['babies','countries','loves','contributed','histories', 'stories', 'stemming', 'going', 'runing', 'carries', 'gone']\n",
    "for i in sent:\n",
    "    print(gen_lemma(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
